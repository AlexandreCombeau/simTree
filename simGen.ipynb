{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "from functools import reduce \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---------------------------------------------------------------------###\n",
    "###-------------------------------------> Helper functions\n",
    "\n",
    "def f(a : str) -> str:\n",
    "    return a+\"o\"\n",
    "def g(a : str) -> str:\n",
    "    return a+\"K\"\n",
    "def h(a : str) -> str:\n",
    "    return a+a\n",
    "def ag1(a : str, b : str) -> str:\n",
    "    return a+\"-\"+b\n",
    "def ag2(a : str, b : str) -> str:\n",
    "    return b+\"-\"+a\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import random\n",
    "\n",
    "rootType = Callable[[str,str], float]\n",
    "nodeType = Callable[[str], str]\n",
    "leafType = [str]\n",
    "\n",
    "class SimTree():\n",
    "\n",
    "    def __init__(self, value : leafType | nodeType | rootType = None, child : list = None):\n",
    "        self.value = value \n",
    "        self.child = child or []\n",
    "        self.isRoot = lambda x: True if(len(x.child) == 2) else False\n",
    "        self.isNode = lambda x: True if(len(x.child) == 1) else False\n",
    "        self.isLeaf = lambda x: True if(len(x.child) == 0) else False\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.isRoot(self): #root \n",
    "            tree_list = str([self.value.__name__,\n",
    "                         self.child[0].__str__(),\n",
    "                         self.child[1].__str__()])\n",
    "        elif self.isNode(self): #nodes\n",
    "            tree_list = [self.value.__name__,self.child[0].__str__()]\n",
    "        else: #leaf\n",
    "            tree_list = self.value\n",
    "        return tree_list  \n",
    "          \n",
    "    def return_tree_asList(self):\n",
    "        if self.isRoot(self): #root \n",
    "            tree_list = [self.value,\n",
    "                         self.child[0].return_tree_asList(),\n",
    "                         self.child[1].return_tree_asList()]\n",
    "        elif self.isNode(self): #nodes\n",
    "            tree_list = [self.value,self.child[0].return_tree_asList()]\n",
    "        else: #leaf\n",
    "            tree_list = self.value \n",
    "        return tree_list\n",
    "      \n",
    "    def compute(self):\n",
    "        if self.isRoot(self): #root\n",
    "            return self.value(self.child[0].compute(),\n",
    "                              self.child[1].compute())\n",
    "        elif self.isNode(self): #nodes\n",
    "            return self.value(self.child[0].compute())\n",
    "        else: #leaf\n",
    "            return self.value\n",
    "    \n",
    "    def mutate(self, get_rd_function, probability : float):\n",
    "        if not self.isLeaf(self):\n",
    "            if random.random() < probability:\n",
    "                self.value = get_rd_function(len(self.child),self.value)\n",
    "            else:\n",
    "                for c in self.child:\n",
    "                    c.mutate(get_rd_function,probability)\n",
    "\n",
    "    def set_leaf_value(self,value):\n",
    "        if self.isNode(self):\n",
    "            self.child[0].set_leaf_value(value)\n",
    "        if self.isLeaf(self):\n",
    "            self.value = value\n",
    "\n",
    "    def set_leafs_value(self,values):\n",
    "        x,y = values\n",
    "        self.child[0].set_leaf_value(x)\n",
    "        self.child[1].set_leaf_value(y)\n",
    "\n",
    "    def get_Similarity_function(self):\n",
    "        if self.isRoot(self): #root\n",
    "            return self.value\n",
    "        \n",
    "    def get_transformations_functions(self):\n",
    "        if self.isRoot(self) == 2: #root\n",
    "            return flatten([self.child[0].get_transformations_functions,\n",
    "                            self.child[1].get_transformations_functions])\n",
    "\n",
    "        elif self.isNode(self) == 1: #nodes\n",
    "            return self.value\n",
    "\n",
    "    def find_depth(self,x):\n",
    "        x+=1\n",
    "        if self.isRoot(self):\n",
    "            return max(self.child[0].find_depth(x), self.child[1].find_depth(x))\n",
    "        if self.isNode(self):\n",
    "            return self.child[0].find_depth(x)\n",
    "        if self.isLeaf(self):\n",
    "            return x    \n",
    "\n",
    "    def get_depth(self):\n",
    "        return self.find_depth(0)    \n",
    "\n",
    "\n",
    "def tree_from_list(tree_list : list) -> SimTree:\n",
    "    if len(tree_list) == 3: #root\n",
    "        value = tree_list[0]\n",
    "        left_child = tree_list[1]\n",
    "        right_child = tree_list[2]\n",
    "        return SimTree(value,\n",
    "                [tree_from_list(left_child),\n",
    "                tree_from_list(right_child)])\n",
    "    elif len(tree_list) == 2: #nodes        \n",
    "        value = tree_list[0]\n",
    "        child = tree_list[1]\n",
    "        return SimTree(value,[tree_from_list(child)])\n",
    "    else: #leaf\n",
    "        value = tree_list[0]\n",
    "        return SimTree(value,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from similarity import *\n",
    "from transformation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMATION_FUNCTIONS = transformation_functions()           \n",
    "SIMILARITY_FUNCTIONS = similarity_functions()\n",
    "           \n",
    "flatten = lambda l : [item for sublist in l for item in sublist]\n",
    "def get_rd_function(nb_child,value):\n",
    "    if nb_child == 2:\n",
    "        function_list = SIMILARITY_FUNCTIONS\n",
    "    else:\n",
    "        function_list = TRANSFORMATION_FUNCTIONS\n",
    "    flag = True\n",
    "    while flag:\n",
    "        new_value = random.choice(function_list)\n",
    "        if new_value != value:\n",
    "            flag = False\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dates = [\n",
    "    '2006-10-28', '2005-11-15', '1936-11-07', '1955-09-08', '1936-12-07',\n",
    "    '1937-04-12', '1974-12-02', '1972-06-30', '1955-07-06', '2006-09-09',\n",
    "    '2007-11-06', '2007-11-13', '2009-09-29', '2014-03-25', '2001-12-04',\n",
    "    '2004-09-06', '2004-11-10', '2004-11-23', '2004-12-09', '2005-09-27',\n",
    "    '2011-09-27', '2000-09-12', '1999-09-14', '2003-11-11', '2004-02-05',\n",
    "    '2006-04-04', '2005-11-16', '2005-11-24', '2008-09-23', '2010-10-19',\n",
    "    '2005-09-13', '2005-09-14', '2006-03-07', '2009-09-08', '2011-11-01',\n",
    "    '2003-06-03', '1969-02-10', '2017-04-28', '2022-10-24', '2004-05-10',\n",
    "    '2018-03-14', '2002-04-11', '2013-03-09'\n",
    "]\n",
    "\n",
    "american_dates = []\n",
    "european_dates = []\n",
    "\n",
    "for date_str in dates:\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    american_date = date_obj.strftime('%m/%d/%Y')\n",
    "    american_dates.append(american_date)\n",
    "    european_date = date_obj.strftime('%d/%m/%Y')\n",
    "    european_dates.append(european_date)\n",
    "\n",
    "# Print the converted dates\n",
    "values = []\n",
    "for a,e in zip(american_dates,european_dates):\n",
    "    #print(a+\" - \"+e)\n",
    "    values.append((a,e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_algo = {\n",
    "    \"population_size\" : 100,\n",
    "    \"nb_generation\"   : 20,\n",
    "    \"proba_mutation\"  : 0.3,\n",
    "    \"proba_crossover\" : 0.2,\n",
    "    #\"proba_ellitism\"  : 0.5,\n",
    "}\n",
    "\n",
    "param_data = {\n",
    "    \"tree_max_depth\" : 8,\n",
    "    \"similarity_functions\" : similarity_functions(),\n",
    "    \"transformation_functions\" : transformation_functions(),\n",
    "    \"values\" : values\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimGen():\n",
    "    def __init__(self, param_algo : dict , param_data : dict):\n",
    "        #main param\n",
    "        self.population_size : int = param_algo[\"population_size\"]\n",
    "        self.nb_generation   : int = param_algo[\"nb_generation\"]\n",
    "        \n",
    "        #evolution param\n",
    "        self.proba_mutation  : float = param_algo[\"proba_mutation\"]\n",
    "        self.proba_crossover : float = param_algo[\"proba_crossover\"]\n",
    "        #self.proba_ellitism  : float = param_algo[\"proba_ellitism\"] #proba to keep best elem\n",
    "\n",
    "        #candidate param\n",
    "        self.tree_max_depth : int = param_data[\"tree_max_depth\"]\n",
    "\n",
    "        #init param\n",
    "        self.population : list[SimTree] = [] #contain every candidate solution\n",
    "        self.population_scores : list[int] = [] #every candidate has a score computed at the end of an generation\n",
    "        self.similarity_functions : list[rootType] = param_data[\"similarity_functions\"] \n",
    "        self.transformation_functions : list[nodeType] = param_data[\"transformation_functions\"]\n",
    "\n",
    "        #values param\n",
    "        self.values : list[tuple[str,str]] = param_data[\"values\"] #[(x1,y1),(x2,y2)...,(xn,yn)] such as (ei,p,xi) <=> (e'i,p',yi)\n",
    "        \n",
    "\n",
    "    def generate_random_tree(self):\n",
    "        nb_transformation = random.randint(1,self.tree_max_depth)\n",
    "        tf_left = np.random.choice(self.transformation_functions,nb_transformation,replace=False)\n",
    "        tf_left = np.append(tf_left,[[\"a\"]]) #TODO check comment mieux gerer le lift\n",
    "        tf_right = np.random.choice(self.transformation_functions,nb_transformation,replace=False)\n",
    "        tf_right = np.append(tf_right,[[\"a\"]])\n",
    "        sf = np.random.choice(self.similarity_functions,1)[0]\n",
    "        def nest_list(lst):\n",
    "            #format our list of function : [f,g,h,...]\n",
    "            #into : [f,[g,[h,[...]]]]\n",
    "            if len(lst) == 1:\n",
    "                return [lst[0]]\n",
    "            else:\n",
    "                return [lst[0], nest_list(lst[1:])]\n",
    "\n",
    "        tree_list = [sf,nest_list(tf_left),nest_list(tf_right)]\n",
    "        tree = tree_from_list(tree_list)\n",
    "        return tree\n",
    "\n",
    "    def init_population(self):\n",
    "        \"\"\"\n",
    "            Generate a population of size n with random tree\n",
    "        \n",
    "        \"\"\"\n",
    "        #check variance and stuff meta data TODO\n",
    "        for _ in range(self.population_size):\n",
    "            self.population.append(self.generate_random_tree())\n",
    "            \n",
    "    def fitness_function(self):\n",
    "        '''\n",
    "            Compute the score of a tree on every value pair\n",
    "            Keep the mean as a score\n",
    "        '''    \n",
    "         \n",
    "        for index,candidate in enumerate(self.population):\n",
    "            for values in self.values:\n",
    "                #compute score for each value pair for one tree\n",
    "                candidate.set_leafs_value(values)\n",
    "                score = candidate.compute()\n",
    "                self.population_scores[index] += score\n",
    "            self.population_scores[index] /= len(self.values)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check how to implement value because don't need value when instance simtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sim \u001b[39m=\u001b[39m SimGen(param_algo,param_data)\n\u001b[0;32m      2\u001b[0m sim\u001b[39m.\u001b[39minit_population()\n\u001b[1;32m----> 3\u001b[0m sim\u001b[39m.\u001b[39;49mfitness_function()\n",
      "Cell \u001b[1;32mIn[34], line 63\u001b[0m, in \u001b[0;36mSimGen.fitness_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m values \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues:\n\u001b[0;32m     61\u001b[0m     \u001b[39m#compute score for each value pair for one tree\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     candidate\u001b[39m.\u001b[39mset_leafs_value(values)\n\u001b[1;32m---> 63\u001b[0m     score \u001b[39m=\u001b[39m candidate\u001b[39m.\u001b[39;49mcompute()\n\u001b[0;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation_scores[index] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m score\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation_scores[index] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues)\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mSimTree.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misRoot(\u001b[39mself\u001b[39m): \u001b[39m#root\u001b[39;00m\n\u001b[0;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcompute(),\n\u001b[1;32m---> 42\u001b[0m                           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchild[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mcompute())\n\u001b[0;32m     43\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misNode(\u001b[39mself\u001b[39m): \u001b[39m#nodes\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcompute())\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mSimTree.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcompute(),\n\u001b[0;32m     42\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcompute())\n\u001b[0;32m     43\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misNode(\u001b[39mself\u001b[39m): \u001b[39m#nodes\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchild[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcompute())\n\u001b[0;32m     45\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39m#leaf\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mSimTree.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcompute(),\n\u001b[0;32m     42\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcompute())\n\u001b[0;32m     43\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misNode(\u001b[39mself\u001b[39m): \u001b[39m#nodes\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchild[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcompute())\n\u001b[0;32m     45\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39m#leaf\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\user2\\Documents\\Stage\\Code\\simTree\\transformation.py:21\u001b[0m, in \u001b[0;36mremove_extra_whitespace\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_extra_whitespace\u001b[39m(x):\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(x\u001b[39m.\u001b[39;49msplit())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "sim = SimGen(param_algo,param_data)\n",
    "sim.init_population()\n",
    "sim.fitness_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['levenshtein_dist_similarity', ['identity', ['remove_extra_whitespace', 'a']], ['remove_stopwords', ['remove_extra_whitespace', 'a']]]\",\n",
       " \"['euclidean_dist_similarity', ['strip_whitespace', 'a'], ['strip_whitespace', 'a']]\",\n",
       " \"['levenshtein_dist_similarity', ['uppercase', ['stem', 'a']], ['tokenize', ['lemmatize', 'a']]]\",\n",
       " \"['euclidean_dist_similarity', ['remove_extra_whitespace', ['lemmatize', 'a']], ['uppercase', ['strip_whitespace', 'a']]]\",\n",
       " \"['jaccard_similarity', ['remove_extra_whitespace', ['uppercase', 'a']], ['identity', ['stem', 'a']]]\",\n",
       " \"['cos_similarity', ['remove_stopwords', ['lowercase', ['strip_whitespace', ['remove_punctuation', ['remove_extra_whitespace', 'a']]]]], ['tokenize', ['uppercase', ['remove_punctuation', ['remove_extra_whitespace', ['strip_whitespace', 'a']]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['lowercase', ['remove_extra_whitespace', ['tokenize', 'a']]], ['remove_extra_whitespace', ['lemmatize', ['strip_whitespace', 'a']]]]\",\n",
       " \"['levenshtein_dist_similarity', ['remove_stopwords', ['remove_extra_whitespace', ['lemmatize', ['tokenize', 'a']]]], ['uppercase', ['stem', ['tokenize', ['strip_whitespace', 'a']]]]]\",\n",
       " \"['pearson_corr_similarity', ['tokenize', ['remove_stopwords', 'a']], ['tokenize', ['uppercase', 'a']]]\",\n",
       " \"['levenshtein_dist_similarity', ['uppercase', ['identity', ['remove_extra_whitespace', ['stem', ['remove_stopwords', ['lemmatize', ['lowercase', 'a']]]]]]], ['tokenize', ['remove_stopwords', ['uppercase', ['remove_punctuation', ['lemmatize', ['stem', ['strip_whitespace', 'a']]]]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['tokenize', ['remove_stopwords', ['identity', ['remove_extra_whitespace', ['lowercase', ['stem', 'a']]]]]], ['uppercase', ['stem', ['remove_punctuation', ['tokenize', ['lemmatize', ['remove_extra_whitespace', 'a']]]]]]]\",\n",
       " \"['cos_similarity', ['lemmatize', ['remove_extra_whitespace', ['strip_whitespace', ['remove_stopwords', ['identity', ['uppercase', ['tokenize', ['stem', 'a']]]]]]]], ['stem', ['uppercase', ['tokenize', ['remove_stopwords', ['identity', ['lowercase', ['remove_punctuation', ['remove_extra_whitespace', 'a']]]]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['remove_punctuation', ['strip_whitespace', ['uppercase', ['tokenize', ['identity', 'a']]]]], ['remove_punctuation', ['lowercase', ['tokenize', ['uppercase', ['strip_whitespace', 'a']]]]]]\",\n",
       " \"['pearson_corr_similarity', ['lowercase', ['identity', ['remove_extra_whitespace', ['lemmatize', ['remove_stopwords', ['remove_punctuation', 'a']]]]]], ['strip_whitespace', ['remove_punctuation', ['lemmatize', ['uppercase', ['stem', ['remove_stopwords', 'a']]]]]]]\",\n",
       " \"['jaccard_similarity', ['identity', ['tokenize', ['stem', ['strip_whitespace', ['uppercase', 'a']]]]], ['strip_whitespace', ['remove_extra_whitespace', ['lowercase', ['remove_stopwords', ['uppercase', 'a']]]]]]\",\n",
       " \"['pearson_corr_similarity', ['remove_extra_whitespace', ['uppercase', ['tokenize', ['lowercase', ['identity', ['remove_punctuation', ['lemmatize', ['strip_whitespace', 'a']]]]]]]], ['tokenize', ['strip_whitespace', ['lowercase', ['identity', ['lemmatize', ['remove_extra_whitespace', ['stem', ['remove_stopwords', 'a']]]]]]]]]\",\n",
       " \"['jaccard_similarity', ['identity', ['uppercase', ['stem', ['remove_punctuation', ['lemmatize', 'a']]]]], ['identity', ['remove_extra_whitespace', ['strip_whitespace', ['tokenize', ['uppercase', 'a']]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['strip_whitespace', ['identity', 'a']], ['tokenize', ['lowercase', 'a']]]\",\n",
       " \"['jaccard_similarity', ['identity', ['strip_whitespace', ['lemmatize', ['lowercase', 'a']]]], ['remove_stopwords', ['remove_extra_whitespace', ['lowercase', ['strip_whitespace', 'a']]]]]\",\n",
       " \"['euclidean_dist_similarity', ['remove_extra_whitespace', 'a'], ['identity', 'a']]\",\n",
       " \"['cos_similarity', ['lemmatize', 'a'], ['remove_extra_whitespace', 'a']]\",\n",
       " \"['euclidean_dist_similarity', ['tokenize', ['remove_stopwords', 'a']], ['remove_stopwords', ['uppercase', 'a']]]\",\n",
       " \"['pearson_corr_similarity', ['lemmatize', ['remove_punctuation', ['stem', ['identity', ['lowercase', ['remove_extra_whitespace', ['uppercase', ['strip_whitespace', 'a']]]]]]]], ['lemmatize', ['strip_whitespace', ['lowercase', ['tokenize', ['remove_punctuation', ['remove_stopwords', ['remove_extra_whitespace', ['identity', 'a']]]]]]]]]\",\n",
       " \"['cos_similarity', ['remove_punctuation', 'a'], ['remove_stopwords', 'a']]\",\n",
       " \"['jaccard_similarity', ['identity', ['strip_whitespace', ['remove_extra_whitespace', ['lowercase', ['tokenize', ['lemmatize', ['remove_stopwords', 'a']]]]]]], ['tokenize', ['uppercase', ['remove_punctuation', ['lowercase', ['remove_stopwords', ['strip_whitespace', ['identity', 'a']]]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['lemmatize', ['remove_stopwords', ['tokenize', ['uppercase', ['remove_extra_whitespace', ['strip_whitespace', 'a']]]]]], ['lemmatize', ['remove_punctuation', ['stem', ['remove_stopwords', ['uppercase', ['lowercase', 'a']]]]]]]\",\n",
       " \"['cos_similarity', ['stem', ['identity', ['remove_extra_whitespace', ['strip_whitespace', ['tokenize', ['lemmatize', ['uppercase', 'a']]]]]]], ['remove_stopwords', ['tokenize', ['remove_punctuation', ['stem', ['identity', ['remove_extra_whitespace', ['strip_whitespace', 'a']]]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['remove_extra_whitespace', 'a'], ['stem', 'a']]\",\n",
       " \"['pearson_corr_similarity', ['stem', ['remove_extra_whitespace', ['remove_stopwords', ['lowercase', ['strip_whitespace', ['identity', 'a']]]]]], ['lowercase', ['uppercase', ['stem', ['remove_extra_whitespace', ['identity', ['remove_stopwords', 'a']]]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['strip_whitespace', ['stem', ['tokenize', ['identity', ['remove_extra_whitespace', ['lemmatize', ['remove_stopwords', ['remove_punctuation', 'a']]]]]]]], ['uppercase', ['lowercase', ['lemmatize', ['remove_stopwords', ['identity', ['remove_punctuation', ['remove_extra_whitespace', ['stem', 'a']]]]]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['stem', 'a'], ['identity', 'a']]\",\n",
       " \"['jaccard_similarity', ['remove_punctuation', ['lowercase', ['lemmatize', ['tokenize', 'a']]]], ['identity', ['remove_extra_whitespace', ['uppercase', ['tokenize', 'a']]]]]\",\n",
       " \"['pearson_corr_similarity', ['uppercase', ['strip_whitespace', ['remove_stopwords', 'a']]], ['uppercase', ['lowercase', ['remove_extra_whitespace', 'a']]]]\",\n",
       " \"['jaccard_similarity', ['remove_punctuation', ['lemmatize', ['uppercase', ['stem', ['tokenize', ['identity', ['remove_extra_whitespace', 'a']]]]]]], ['lowercase', ['tokenize', ['remove_stopwords', ['strip_whitespace', ['stem', ['uppercase', ['remove_punctuation', 'a']]]]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['strip_whitespace', ['remove_stopwords', 'a']], ['stem', ['remove_punctuation', 'a']]]\",\n",
       " \"['pearson_corr_similarity', ['stem', ['lemmatize', ['remove_extra_whitespace', ['strip_whitespace', ['identity', ['remove_punctuation', 'a']]]]]], ['remove_extra_whitespace', ['lowercase', ['uppercase', ['lemmatize', ['strip_whitespace', ['remove_stopwords', 'a']]]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['uppercase', ['strip_whitespace', ['lemmatize', 'a']]], ['identity', ['lemmatize', ['stem', 'a']]]]\",\n",
       " \"['jaccard_similarity', ['strip_whitespace', ['lemmatize', ['tokenize', ['stem', ['lowercase', ['identity', ['remove_stopwords', ['remove_extra_whitespace', 'a']]]]]]]], ['uppercase', ['strip_whitespace', ['lemmatize', ['stem', ['remove_stopwords', ['lowercase', ['remove_punctuation', ['identity', 'a']]]]]]]]]\",\n",
       " \"['cos_similarity', ['remove_stopwords', ['lowercase', ['remove_punctuation', ['remove_extra_whitespace', ['uppercase', 'a']]]]], ['lowercase', ['strip_whitespace', ['remove_stopwords', ['identity', ['tokenize', 'a']]]]]]\",\n",
       " \"['cos_similarity', ['tokenize', ['strip_whitespace', 'a']], ['identity', ['remove_punctuation', 'a']]]\",\n",
       " \"['jaccard_similarity', ['lowercase', ['lemmatize', ['stem', ['uppercase', 'a']]]], ['remove_extra_whitespace', ['identity', ['remove_punctuation', ['stem', 'a']]]]]\",\n",
       " \"['euclidean_dist_similarity', ['stem', 'a'], ['lowercase', 'a']]\",\n",
       " \"['jaccard_similarity', ['tokenize', ['stem', ['remove_stopwords', ['identity', ['strip_whitespace', 'a']]]]], ['identity', ['lemmatize', ['strip_whitespace', ['tokenize', ['stem', 'a']]]]]]\",\n",
       " \"['jaccard_similarity', ['strip_whitespace', 'a'], ['stem', 'a']]\",\n",
       " \"['cos_similarity', ['remove_punctuation', ['lemmatize', ['lowercase', ['tokenize', ['strip_whitespace', ['uppercase', 'a']]]]]], ['lemmatize', ['uppercase', ['lowercase', ['tokenize', ['identity', ['remove_punctuation', 'a']]]]]]]\",\n",
       " \"['jaccard_similarity', ['remove_stopwords', ['lowercase', ['remove_punctuation', ['uppercase', ['tokenize', ['lemmatize', ['strip_whitespace', 'a']]]]]]], ['lowercase', ['remove_stopwords', ['uppercase', ['remove_extra_whitespace', ['lemmatize', ['tokenize', ['strip_whitespace', 'a']]]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['remove_stopwords', ['strip_whitespace', 'a']], ['lowercase', ['stem', 'a']]]\",\n",
       " \"['jaccard_similarity', ['strip_whitespace', ['lemmatize', ['uppercase', ['tokenize', ['remove_extra_whitespace', ['remove_punctuation', ['identity', ['stem', 'a']]]]]]]], ['stem', ['strip_whitespace', ['lowercase', ['remove_stopwords', ['tokenize', ['identity', ['remove_extra_whitespace', ['uppercase', 'a']]]]]]]]]\",\n",
       " \"['cos_similarity', ['uppercase', ['remove_stopwords', ['lemmatize', ['tokenize', 'a']]]], ['identity', ['stem', ['lowercase', ['lemmatize', 'a']]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['strip_whitespace', ['identity', ['uppercase', 'a']]], ['remove_extra_whitespace', ['remove_punctuation', ['uppercase', 'a']]]]\",\n",
       " \"['pearson_corr_similarity', ['lemmatize', ['tokenize', ['identity', ['stem', ['remove_punctuation', 'a']]]]], ['identity', ['lowercase', ['strip_whitespace', ['remove_punctuation', ['stem', 'a']]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['remove_stopwords', ['lemmatize', ['remove_extra_whitespace', ['lowercase', ['stem', ['strip_whitespace', 'a']]]]]], ['remove_extra_whitespace', ['strip_whitespace', ['remove_stopwords', ['uppercase', ['tokenize', ['remove_punctuation', 'a']]]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['remove_stopwords', 'a'], ['stem', 'a']]\",\n",
       " \"['jaccard_similarity', ['lemmatize', ['identity', ['remove_stopwords', ['uppercase', ['remove_punctuation', ['strip_whitespace', 'a']]]]]], ['remove_punctuation', ['uppercase', ['remove_extra_whitespace', ['identity', ['strip_whitespace', ['remove_stopwords', 'a']]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['lemmatize', ['tokenize', 'a']], ['uppercase', ['lowercase', 'a']]]\",\n",
       " \"['euclidean_dist_similarity', ['uppercase', 'a'], ['stem', 'a']]\",\n",
       " \"['jaccard_similarity', ['lowercase', ['uppercase', ['identity', ['remove_extra_whitespace', ['lemmatize', ['strip_whitespace', ['remove_stopwords', 'a']]]]]]], ['tokenize', ['stem', ['strip_whitespace', ['uppercase', ['remove_extra_whitespace', ['identity', ['remove_stopwords', 'a']]]]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['lemmatize', 'a'], ['remove_stopwords', 'a']]\",\n",
       " \"['levenshtein_dist_similarity', ['remove_extra_whitespace', ['remove_stopwords', ['tokenize', ['lowercase', ['strip_whitespace', ['uppercase', ['identity', ['lemmatize', 'a']]]]]]]], ['remove_punctuation', ['uppercase', ['remove_extra_whitespace', ['lowercase', ['strip_whitespace', ['stem', ['tokenize', ['identity', 'a']]]]]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['lowercase', ['strip_whitespace', 'a']], ['remove_punctuation', ['identity', 'a']]]\",\n",
       " \"['levenshtein_dist_similarity', ['remove_stopwords', ['lowercase', ['remove_punctuation', ['identity', ['tokenize', 'a']]]]], ['strip_whitespace', ['tokenize', ['uppercase', ['identity', ['lowercase', 'a']]]]]]\",\n",
       " \"['pearson_corr_similarity', ['strip_whitespace', ['stem', ['uppercase', ['lowercase', ['lemmatize', ['identity', ['tokenize', ['remove_stopwords', 'a']]]]]]]], ['remove_punctuation', ['stem', ['lemmatize', ['identity', ['strip_whitespace', ['uppercase', ['tokenize', ['remove_extra_whitespace', 'a']]]]]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['tokenize', ['lemmatize', 'a']], ['stem', ['strip_whitespace', 'a']]]\",\n",
       " \"['pearson_corr_similarity', ['strip_whitespace', ['uppercase', ['lowercase', ['remove_extra_whitespace', 'a']]]], ['lowercase', ['remove_extra_whitespace', ['lemmatize', ['uppercase', 'a']]]]]\",\n",
       " \"['euclidean_dist_similarity', ['tokenize', ['strip_whitespace', ['remove_stopwords', ['identity', 'a']]]], ['strip_whitespace', ['tokenize', ['remove_stopwords', ['remove_extra_whitespace', 'a']]]]]\",\n",
       " \"['cos_similarity', ['lemmatize', ['stem', ['lowercase', 'a']]], ['stem', ['remove_punctuation', ['remove_extra_whitespace', 'a']]]]\",\n",
       " \"['levenshtein_dist_similarity', ['identity', ['stem', ['remove_stopwords', ['tokenize', ['remove_punctuation', ['strip_whitespace', ['remove_extra_whitespace', ['uppercase', 'a']]]]]]]], ['remove_punctuation', ['remove_stopwords', ['lowercase', ['uppercase', ['stem', ['tokenize', ['identity', ['lemmatize', 'a']]]]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['lemmatize', ['strip_whitespace', ['remove_punctuation', ['lowercase', ['remove_stopwords', ['stem', ['tokenize', 'a']]]]]]], ['strip_whitespace', ['remove_extra_whitespace', ['tokenize', ['lemmatize', ['stem', ['remove_stopwords', ['uppercase', 'a']]]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['lowercase', ['tokenize', ['lemmatize', ['identity', ['remove_extra_whitespace', ['remove_punctuation', ['strip_whitespace', 'a']]]]]]], ['identity', ['uppercase', ['remove_punctuation', ['remove_stopwords', ['tokenize', ['stem', ['lemmatize', 'a']]]]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['lowercase', ['remove_stopwords', ['strip_whitespace', ['uppercase', 'a']]]], ['tokenize', ['uppercase', ['stem', ['remove_punctuation', 'a']]]]]\",\n",
       " \"['euclidean_dist_similarity', ['identity', ['remove_punctuation', ['lowercase', ['remove_extra_whitespace', ['strip_whitespace', ['remove_stopwords', ['tokenize', ['stem', 'a']]]]]]]], ['uppercase', ['remove_extra_whitespace', ['identity', ['tokenize', ['stem', ['remove_punctuation', ['remove_stopwords', ['strip_whitespace', 'a']]]]]]]]]\",\n",
       " \"['jaccard_similarity', ['remove_punctuation', ['uppercase', 'a']], ['tokenize', ['strip_whitespace', 'a']]]\",\n",
       " \"['cos_similarity', ['remove_punctuation', ['strip_whitespace', ['tokenize', 'a']]], ['identity', ['tokenize', ['strip_whitespace', 'a']]]]\",\n",
       " \"['euclidean_dist_similarity', ['remove_stopwords', ['strip_whitespace', ['remove_punctuation', ['lemmatize', 'a']]]], ['identity', ['lemmatize', ['remove_extra_whitespace', ['lowercase', 'a']]]]]\",\n",
       " \"['cos_similarity', ['lemmatize', ['remove_extra_whitespace', ['remove_stopwords', ['tokenize', 'a']]]], ['remove_punctuation', ['stem', ['lowercase', ['remove_stopwords', 'a']]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['identity', ['remove_extra_whitespace', ['stem', 'a']]], ['tokenize', ['stem', ['remove_extra_whitespace', 'a']]]]\",\n",
       " \"['levenshtein_dist_similarity', ['remove_extra_whitespace', ['lowercase', ['remove_punctuation', ['stem', 'a']]]], ['uppercase', ['remove_punctuation', ['stem', ['lowercase', 'a']]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['strip_whitespace', ['lowercase', ['remove_punctuation', ['uppercase', ['lemmatize', ['remove_extra_whitespace', ['stem', 'a']]]]]]], ['lowercase', ['remove_extra_whitespace', ['stem', ['remove_stopwords', ['identity', ['remove_punctuation', ['strip_whitespace', 'a']]]]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['identity', ['strip_whitespace', ['remove_extra_whitespace', 'a']]], ['remove_stopwords', ['remove_extra_whitespace', ['lemmatize', 'a']]]]\",\n",
       " \"['levenshtein_dist_similarity', ['stem', 'a'], ['remove_punctuation', 'a']]\",\n",
       " \"['pearson_corr_similarity', ['remove_extra_whitespace', ['lowercase', ['uppercase', ['remove_punctuation', 'a']]]], ['remove_extra_whitespace', ['lemmatize', ['stem', ['lowercase', 'a']]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['identity', ['uppercase', ['lowercase', ['stem', ['remove_punctuation', ['remove_stopwords', ['lemmatize', ['strip_whitespace', 'a']]]]]]]], ['lemmatize', ['stem', ['identity', ['tokenize', ['strip_whitespace', ['remove_extra_whitespace', ['lowercase', ['uppercase', 'a']]]]]]]]]\",\n",
       " \"['jaccard_similarity', ['remove_stopwords', ['tokenize', ['lemmatize', ['stem', ['lowercase', ['strip_whitespace', ['uppercase', ['remove_punctuation', 'a']]]]]]]], ['lowercase', ['tokenize', ['identity', ['remove_punctuation', ['strip_whitespace', ['stem', ['lemmatize', ['uppercase', 'a']]]]]]]]]\",\n",
       " \"['cos_similarity', ['remove_extra_whitespace', ['remove_stopwords', 'a']], ['uppercase', ['stem', 'a']]]\",\n",
       " \"['jaccard_similarity', ['identity', 'a'], ['tokenize', 'a']]\",\n",
       " \"['cos_similarity', ['uppercase', ['remove_extra_whitespace', ['remove_punctuation', ['remove_stopwords', ['stem', 'a']]]]], ['lowercase', ['remove_extra_whitespace', ['uppercase', ['remove_stopwords', ['identity', 'a']]]]]]\",\n",
       " \"['euclidean_dist_similarity', ['uppercase', 'a'], ['strip_whitespace', 'a']]\",\n",
       " \"['cos_similarity', ['strip_whitespace', ['remove_extra_whitespace', ['lemmatize', 'a']]], ['remove_punctuation', ['stem', ['strip_whitespace', 'a']]]]\",\n",
       " \"['pearson_corr_similarity', ['lowercase', ['uppercase', 'a']], ['remove_punctuation', ['remove_extra_whitespace', 'a']]]\",\n",
       " \"['cos_similarity', ['remove_stopwords', ['remove_extra_whitespace', 'a']], ['uppercase', ['lemmatize', 'a']]]\",\n",
       " \"['cos_similarity', ['tokenize', ['remove_punctuation', ['strip_whitespace', ['identity', ['lowercase', 'a']]]]], ['strip_whitespace', ['remove_extra_whitespace', ['tokenize', ['lowercase', ['remove_punctuation', 'a']]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['remove_extra_whitespace', ['stem', ['identity', ['remove_stopwords', ['strip_whitespace', 'a']]]]], ['remove_stopwords', ['remove_punctuation', ['tokenize', ['remove_extra_whitespace', ['stem', 'a']]]]]]\",\n",
       " \"['cos_similarity', ['strip_whitespace', ['identity', ['lowercase', ['remove_stopwords', ['remove_punctuation', ['uppercase', ['remove_extra_whitespace', 'a']]]]]]], ['tokenize', ['lowercase', ['lemmatize', ['remove_extra_whitespace', ['stem', ['remove_punctuation', ['identity', 'a']]]]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['lowercase', ['strip_whitespace', ['tokenize', ['lemmatize', ['identity', 'a']]]]], ['tokenize', ['lowercase', ['stem', ['remove_stopwords', ['remove_punctuation', 'a']]]]]]\",\n",
       " \"['cos_similarity', ['remove_extra_whitespace', ['uppercase', ['stem', ['remove_punctuation', ['remove_stopwords', ['lowercase', 'a']]]]]], ['remove_stopwords', ['identity', ['strip_whitespace', ['remove_extra_whitespace', ['lemmatize', ['remove_punctuation', 'a']]]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['uppercase', ['remove_punctuation', ['tokenize', ['strip_whitespace', 'a']]]], ['stem', ['lemmatize', ['strip_whitespace', ['uppercase', 'a']]]]]\",\n",
       " \"['cos_similarity', ['identity', ['remove_punctuation', ['remove_extra_whitespace', 'a']]], ['lowercase', ['strip_whitespace', ['remove_extra_whitespace', 'a']]]]\",\n",
       " \"['levenshtein_dist_similarity', ['lemmatize', ['identity', ['remove_stopwords', ['lowercase', ['stem', ['remove_extra_whitespace', ['uppercase', 'a']]]]]]], ['remove_stopwords', ['uppercase', ['remove_punctuation', ['identity', ['lemmatize', ['lowercase', ['stem', 'a']]]]]]]]\",\n",
       " \"['pearson_corr_similarity', ['strip_whitespace', ['identity', ['lemmatize', ['stem', ['uppercase', ['tokenize', 'a']]]]]], ['lemmatize', ['tokenize', ['remove_punctuation', ['remove_extra_whitespace', ['strip_whitespace', ['stem', 'a']]]]]]]\",\n",
       " \"['levenshtein_dist_similarity', ['remove_extra_whitespace', ['uppercase', ['tokenize', ['lemmatize', ['identity', ['strip_whitespace', ['lowercase', 'a']]]]]]], ['uppercase', ['remove_stopwords', ['stem', ['tokenize', ['lowercase', ['remove_extra_whitespace', ['lemmatize', 'a']]]]]]]]\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: str(x),sim.population))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '3', ''], dtype='<U11')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = np.array([1,2,3])\n",
    "foo = np.append(foo,[\"\"])\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aKo-bKKK\n",
      "['ag1', ['f', ['g', 'a']], ['g', ['g', ['g', 'b']]]]\n",
      "3\n",
      "['levenshtein_dist_similarity', ['f', ['g', 'a']], ['g', ['g', ['g', 'b']]]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = [ag1,[f,[g,\"a\"]],[g,[g,[g,[\"b\"]]]]]\n",
    "tree = tree_from_list(a)\n",
    "print(tree.compute())\n",
    "print(tree)\n",
    "tree.mutate(get_rd_function,0.4)\n",
    "print(tree.compute())\n",
    "print(tree)\n",
    "print(tree.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ag1', ['f', ['g', '']], ['g', ['g', '']]]\n",
      "['ag1', ['f', ['g', 'b']], ['g', ['g', 'c']]]\n"
     ]
    }
   ],
   "source": [
    "a = [ag1,[f,[g,[\"\"]]],[g,[g,[\"\"]]]]\n",
    "tree = tree_from_list(a)\n",
    "print(tree)\n",
    "tree.set_leafs_value([\"b\",\"c\"])\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 11, 17]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = [(1,2),(4,5),(3,8),(8,9)]\n",
    "compute_score = lambda elem:(x:=[elem[0],y:=[elem[1]]], x[0]+y[0])[-1]\n",
    "list(map(compute_score,k))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
